{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4863169,"sourceType":"datasetVersion","datasetId":2819365}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport kagglehub\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport pathlib\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.applications import EfficientNetV2B1\nfrom tensorflow.keras.applications.efficientnet_v2 import preprocess_input\nfrom tensorflow.keras import regularizers\nfrom sklearn.model_selection import train_test_split\nimport cv2\n\nkaggle_wound_path = kagglehub.dataset_download('yasinpratomo/wound-dataset')\ndataset_path = f\"{kaggle_wound_path}/Wound_dataset\"\nprint(dataset_path)\n\nfor d in dataset_path:\n  print(d)\n\nfor root, dirs, files in os.walk(dataset_path):\n  # print(\"Directory: \", root)\n  for filename in files:\n    full_filepath = os.path.join(root, filename)\n    # print(f\" = {full_filepath}\")","metadata":{"id":"kaQB4xDCHych","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.706534Z","iopub.status.idle":"2025-07-18T21:32:28.706853Z","shell.execute_reply.started":"2025-07-18T21:32:28.706690Z","shell.execute_reply":"2025-07-18T21:32:28.706705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nimport random\nfrom pathlib import Path\n\n# CONFIGURATION\noriginal_dataset_dir = dataset_path\noutput_base_dir = \"dataset_split\"\ntrain_ratio = 0.7\nval_ratio = 0.15\ntest_ratio = 0.15\nSEED = 123\n\nrandom.seed(SEED)\n\n# Make output folders\nfor split in [\"train\", \"val\", \"test\"]:\n    split_path = Path(output_base_dir) / split\n    split_path.mkdir(parents=True, exist_ok=True)\n\n# Go through each class\nfor class_name in os.listdir(original_dataset_dir):\n    class_dir = os.path.join(original_dataset_dir, class_name)\n    if not os.path.isdir(class_dir):\n        continue  # skip non-folder items\n\n    images = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n    random.shuffle(images)\n\n    total = len(images)\n    train_cutoff = int(train_ratio * total)\n    val_cutoff = train_cutoff + int(val_ratio * total)\n\n    train_images = images[:train_cutoff]\n    val_images = images[train_cutoff:val_cutoff]\n    test_images = images[val_cutoff:]\n\n    def move_images(image_list, split):\n        split_class_dir = Path(output_base_dir) / split / class_name\n        split_class_dir.mkdir(parents=True, exist_ok=True)\n        for img_name in image_list:\n            src = os.path.join(class_dir, img_name)\n            dst = os.path.join(split_class_dir, img_name)\n            shutil.copy2(src, dst)\n\n    move_images(train_images, \"train\")\n    move_images(val_images, \"val\")\n    move_images(test_images, \"test\")\n\n    print(f\"Class '{class_name}': {len(train_images)} train, {len(val_images)} val, {len(test_images)} test\")\n\nprint(\"Dataset split complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.708048Z","iopub.status.idle":"2025-07-18T21:32:28.708313Z","shell.execute_reply.started":"2025-07-18T21:32:28.708197Z","shell.execute_reply":"2025-07-18T21:32:28.708210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"id":"jUGOXrvhWfXs","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.709201Z","iopub.status.idle":"2025-07-18T21:32:28.709498Z","shell.execute_reply.started":"2025-07-18T21:32:28.709353Z","shell.execute_reply":"2025-07-18T21:32:28.709369Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## image mode = RGB -> 3 channels","metadata":{"id":"o3bytGUqLcPm"}},{"cell_type":"code","source":"def preprocess(img):\n  if img.shape[2] == 3:\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n  img = cv2.resize(img, (224, 224))\n  img = img.astype(\"float32\")\n\n  img = preprocess_input(img)\n  img = np.expand_dims(img, axis=0)\n  return img","metadata":{"id":"gtYL9f6mLY7b","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.710613Z","iopub.status.idle":"2025-07-18T21:32:28.710835Z","shell.execute_reply.started":"2025-07-18T21:32:28.710733Z","shell.execute_reply":"2025-07-18T21:32:28.710743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 32\nIMG_WIDTH = 180\nIMG_HEIGHT = 180","metadata":{"id":"TUvCQGVmNUHn","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.711819Z","iopub.status.idle":"2025-07-18T21:32:28.712017Z","shell.execute_reply.started":"2025-07-18T21:32:28.711922Z","shell.execute_reply":"2025-07-18T21:32:28.711931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_set = keras.utils.image_dataset_from_directory(\n    \"dataset_split/train\",\n    image_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE\n)","metadata":{"id":"jmJVK75wSmwk","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.712519Z","iopub.status.idle":"2025-07-18T21:32:28.712819Z","shell.execute_reply.started":"2025-07-18T21:32:28.712665Z","shell.execute_reply":"2025-07-18T21:32:28.712682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"validation_set = keras.utils.image_dataset_from_directory(\n    \"dataset_split/val\",\n    image_size=(IMG_HEIGHT, IMG_HEIGHT),\n    batch_size=BATCH_SIZE\n)","metadata":{"id":"PFcuRMM3ttIA","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.714111Z","iopub.status.idle":"2025-07-18T21:32:28.714324Z","shell.execute_reply.started":"2025-07-18T21:32:28.714227Z","shell.execute_reply":"2025-07-18T21:32:28.714236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_set = keras.utils.image_dataset_from_directory(\n    \"dataset_split/test\",\n    image_size=(IMG_HEIGHT, IMG_HEIGHT),\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.715916Z","iopub.status.idle":"2025-07-18T21:32:28.716241Z","shell.execute_reply.started":"2025-07-18T21:32:28.716084Z","shell.execute_reply":"2025-07-18T21:32:28.716100Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data augmentation as there was alot of overfitting (train doing well, val doing not well)","metadata":{"id":"NjgIo13-WfXv"}},{"cell_type":"code","source":"# data_augmentation = keras.Sequential([\n#     layers.RandomFlip(\"horizontal_and_vertical\"),\n#     layers.RandomRotation(0.2),\n#     layers.RandomZoom(0.1),\n#     layers.RandomContrast(0.2),\n# ])\n\ndata_augmentation = keras.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.1),\n    layers.RandomZoom(0.1),\n    layers.RandomContrast(0.1),\n    layers.RandomTranslation(0.1, 0.1),\n])\n","metadata":{"id":"c522flBiWfXw","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.717097Z","iopub.status.idle":"2025-07-18T21:32:28.717403Z","shell.execute_reply.started":"2025-07-18T21:32:28.717241Z","shell.execute_reply":"2025-07-18T21:32:28.717256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = train_set.class_names\nprint(class_names)","metadata":{"id":"1A63sFObu16V","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.718409Z","iopub.status.idle":"2025-07-18T21:32:28.718691Z","shell.execute_reply.started":"2025-07-18T21:32:28.718557Z","shell.execute_reply":"2025-07-18T21:32:28.718573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_set.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")","metadata":{"id":"tiwYzeOsu81e","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.719348Z","iopub.status.idle":"2025-07-18T21:32:28.719659Z","shell.execute_reply.started":"2025-07-18T21:32:28.719500Z","shell.execute_reply":"2025-07-18T21:32:28.719513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for image_batch, labels_batch in train_set:\n  print(f\"IMAGE BATCH SHAPE: {image_batch} <-> LABELS BATCH SHAPE: {labels_batch}\")","metadata":{"id":"MQJNZKtru_zV","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.721380Z","iopub.status.idle":"2025-07-18T21:32:28.721620Z","shell.execute_reply.started":"2025-07-18T21:32:28.721516Z","shell.execute_reply":"2025-07-18T21:32:28.721526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\ntrain_set = train_set.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nvalidation_set = validation_set.cache().prefetch(buffer_size=AUTOTUNE)","metadata":{"id":"yMTZRkJOxd9x","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.722931Z","iopub.status.idle":"2025-07-18T21:32:28.723252Z","shell.execute_reply.started":"2025-07-18T21:32:28.723096Z","shell.execute_reply":"2025-07-18T21:32:28.723109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install optuna","metadata":{"id":"FIUe2tD3rqlo","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.724520Z","iopub.status.idle":"2025-07-18T21:32:28.724820Z","shell.execute_reply.started":"2025-07-18T21:32:28.724668Z","shell.execute_reply":"2025-07-18T21:32:28.724683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom keras.layers import GlobalAveragePooling2D, BatchNormalization, Dropout, Dense\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\ntf.keras.backend.clear_session()\n\ndef objective(trial):\n  optim_learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2)\n  optim_dropout_rate = trial.suggest_float(\"dropout\", 0.2, 0.5)\n  optim_weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2)\n  optim_patience = trial.suggest_int(\"patience\", 5, 10)\n\n  num_classes = len(class_names)\n\n\n  # base_model = EfficientNetV2B0(include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), weights='imagenet')\n  # base_model.trainable = True\n  # for layer in base_model.layers[:-20]:\n  #   layer.trainable = False\n\n  # inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n  # x = data_augmentation(inputs)\n  # x = preprocess_input(x)\n  # x = base_model(x, training=False)\n  # x = layers.GlobalAveragePooling2D()(x)\n  # x = layers.Dropout(optim_dropout_rate)(x)\n  # outputs = layers.Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(optim_weight_decay))(x)\n  base_model = EfficientNetV2B1(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, \n                                                                                    IMG_WIDTH, 3))\n    \n  # Freezing all layers except the last 10\n  for layer in base_model.layers[:-10]:  # Fine-tune more layers\n    layer.trainable = False\n    \n    # Build the model\n  vision_network = Sequential([\n        base_model,\n        GlobalAveragePooling2D(),\n        BatchNormalization(),\n        Dense(512, activation='relu'),\n        Dropout(optim_dropout_rate),\n        Dense(num_classes, activation='softmax')  # Output layer\n  ])\n\n  vision_network.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=optim_learning_rate),\n                        loss=keras.losses.SparseCategoricalCrossentropy(),\n                        metrics=[\"accuracy\"])\n\n  print(vision_network.summary())\n\n  EPOCHS = 50\n\n  lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n  early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n    # setting unique model checkpoint path for each trial (because if overwriting same path, gives error and best hyperparameters won't save)\n  checkpoint_path = f\"best_model_trial_{trial.number}.keras\"\n  checkpoint = keras.callbacks.ModelCheckpoint(\n  checkpoint_path, save_best_only=True, monitor=\"val_loss\", mode=\"min\"\n  )\n\n  history = vision_network.fit(\n      train_set,\n      validation_data=validation_set,\n      epochs=EPOCHS,\n      callbacks=[lr_scheduler, early_stopping, checkpoint],\n  )\n\n  return min(history.history[\"val_loss\"])\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=20)","metadata":{"id":"-eri7AmB9q2J","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.725994Z","iopub.status.idle":"2025-07-18T21:32:28.726316Z","shell.execute_reply.started":"2025-07-18T21:32:28.726149Z","shell.execute_reply":"2025-07-18T21:32:28.726162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_trial_number = study.best_trial.number\nbest_model_path = f\"best_model_trial_{best_trial_number}.keras\"\n\nbest_model = tf.keras.models.load_model(best_model_path)\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(best_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()\n\nwith open(\"model.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n\nprint(f\"TFLite model saved as model.tflite (from trial #{best_trial_number})\")\n","metadata":{"id":"ZZkPuedEvHmj","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.726974Z","iopub.status.idle":"2025-07-18T21:32:28.727267Z","shell.execute_reply.started":"2025-07-18T21:32:28.727143Z","shell.execute_reply":"2025-07-18T21:32:28.727157Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## reusing best params to compile model and retraining ->  then plotting accuracy","metadata":{"id":"fl5geCq8aL_1"}},{"cell_type":"code","source":"best_parameters = study.best_trial.params\n\n# rebuild the model using the best parameters found\nbase_model = EfficientNetV2B0(include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), weights='imagenet')\nbase_model.trainable = False\n\ninputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\nx = data_augmentation(inputs)\nx = preprocess_input(x)\nx = base_model(x, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dropout(best_parameters[\"dropout\"])(x)\noutputs = layers.Dense(\n    len(class_names), \n    activation='softmax', \n    kernel_regularizer=regularizers.l2(best_parameters[\"weight_decay\"])\n)(x)\n\nvision_network = keras.Model(inputs, outputs)\n\ntf.keras.backend.clear_session()\nvision_network.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=best_parameters[\"learning_rate\"]),\n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[\"accuracy\"]\n)\n\nlr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\nhistory = vision_network.fit(\n    train_set,\n    validation_data=validation_set,\n    epochs=100,\n    callbacks=[lr_scheduler, early_stopping],\n    verbose=1\n)\n\naccuracy = history.history[\"accuracy\"]\nvalidation_accuracy = history.history[\"val_accuracy\"]\nloss = history.history[\"loss\"]\nvalidation_loss = history.history[\"val_loss\"]\nepochs_range = range(len(accuracy))\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, accuracy, label=\"Training Accuracy\")\nplt.plot(epochs_range, validation_accuracy, label=\"Validation Accuracy\")\nplt.legend(loc=\"lower right\")\nplt.title(\"Training and Validation Accuracy\")\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label=\"Training Loss\")\nplt.plot(epochs_range, validation_loss, label=\"Validation Loss\")\nplt.legend(loc=\"upper right\")\nplt.title(\"Training and Validation Loss\")\nplt.show()\n\n\nvision_network.save(\"final_model.keras\")\n\n# converting to TFLite for loading into mobile app\nconverter = tf.lite.TFLiteConverter.from_keras_model(vision_network)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()\n\nwith open(\"final_model.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n\nprint(\"TFLite model saved as final_model.tflite\")\n","metadata":{"id":"W6oLSwUYAgO2","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.728477Z","iopub.status.idle":"2025-07-18T21:32:28.728858Z","shell.execute_reply.started":"2025-07-18T21:32:28.728675Z","shell.execute_reply":"2025-07-18T21:32:28.728692Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluating on test set","metadata":{}},{"cell_type":"code","source":"test_loss, test_accuracy = vision_network.evaluate(test_set)\nprint(f\"Test accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:32:28.729990Z","iopub.status.idle":"2025-07-18T21:32:28.730358Z","shell.execute_reply.started":"2025-07-18T21:32:28.730175Z","shell.execute_reply":"2025-07-18T21:32:28.730189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}